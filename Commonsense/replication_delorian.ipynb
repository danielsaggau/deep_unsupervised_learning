{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replication_delorian.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/deep_unsupervised_learning/blob/main/replication_delorian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkg60cbrlJyv",
        "outputId": "cc432937-eb1e-460c-fd7d-d73d153e9d9f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 12 20:02:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ttiBiUvxrx",
        "outputId": "323885eb-85f5-4411-a6de-d8e91c62c5bc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw3YXeTP3squ",
        "outputId": "23d63e7f-7ef2-4b3f-dbac-f6a46224bfcf"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/unsup_gen_for_cms_reasoning"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/unsup_gen_for_cms_reasoning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Kp7HDsqBVq"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAYAKZSBWm0"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLKM0NvBTO-"
      },
      "source": [
        "!sh run_counterfactual_main.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on5ReVE_m3_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752ba629-8a84-4d6c-9d0a-6e3eaf220348"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/unsup_gen_for_cms_reasoning\n",
        "!sh run_abductive_main.sh"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/unsup_gen_for_cms_reasoning\n",
            "100% 20/20 [11:38<00:00, 34.90s/it]\n",
            "100% 20/20 [11:36<00:00, 34.83s/it]\n",
            "100% 20/20 [10:29<00:00, 31.50s/it]\n",
            "100% 20/20 [10:57<00:00, 32.88s/it]\n",
            "100% 20/20 [11:39<00:00, 34.97s/it]\n",
            "100% 20/20 [10:57<00:00, 32.88s/it]\n",
            "100% 20/20 [11:43<00:00, 35.19s/it]\n",
            "100% 20/20 [10:56<00:00, 32.85s/it]\n",
            "100% 20/20 [09:55<00:00, 29.79s/it]\n",
            "100% 20/20 [11:25<00:00, 34.26s/it]\n",
            "100% 20/20 [10:48<00:00, 32.45s/it]\n",
            "100% 20/20 [10:55<00:00, 32.76s/it]\n",
            "100% 20/20 [10:07<00:00, 30.37s/it]\n",
            "100% 20/20 [10:59<00:00, 32.99s/it]\n",
            "100% 20/20 [10:50<00:00, 32.52s/it]\n",
            "100% 20/20 [10:50<00:00, 32.51s/it]\n",
            "100% 20/20 [10:19<00:00, 30.95s/it]\n",
            "100% 20/20 [10:56<00:00, 32.83s/it]\n",
            "100% 20/20 [10:04<00:00, 30.24s/it]\n",
            "100% 20/20 [11:12<00:00, 33.62s/it]\n",
            "100% 20/20 [11:07<00:00, 33.36s/it]\n",
            "100% 20/20 [11:00<00:00, 33.02s/it]\n",
            "100% 20/20 [11:03<00:00, 33.18s/it]\n",
            "100% 20/20 [12:04<00:00, 36.24s/it]\n",
            "100% 20/20 [11:17<00:00, 33.90s/it]\n",
            "100% 20/20 [10:01<00:00, 30.10s/it]\n",
            "100% 20/20 [11:06<00:00, 33.34s/it]\n",
            "100% 20/20 [11:15<00:00, 33.79s/it]\n",
            "100% 20/20 [11:33<00:00, 34.69s/it]\n",
            "100% 20/20 [11:10<00:00, 33.51s/it]\n",
            "100% 20/20 [10:35<00:00, 31.76s/it]\n",
            "100% 20/20 [10:53<00:00, 32.67s/it]\n",
            "100% 20/20 [10:46<00:00, 32.34s/it]\n",
            "100% 20/20 [10:36<00:00, 31.82s/it]\n",
            "100% 20/20 [11:11<00:00, 33.57s/it]\n",
            "100% 20/20 [09:45<00:00, 29.28s/it]\n",
            "100% 20/20 [10:24<00:00, 31.21s/it]\n",
            "100% 20/20 [10:26<00:00, 31.32s/it]\n",
            "100% 20/20 [10:51<00:00, 32.60s/it]\n",
            "100% 20/20 [10:45<00:00, 32.28s/it]\n",
            "100% 20/20 [10:51<00:00, 32.60s/it]\n",
            "100% 20/20 [10:35<00:00, 31.79s/it]\n",
            "100% 20/20 [09:46<00:00, 29.32s/it]\n",
            "100% 20/20 [10:35<00:00, 31.80s/it]\n",
            "100% 20/20 [09:45<00:00, 29.29s/it]\n",
            "100% 20/20 [10:37<00:00, 31.87s/it]\n",
            "100% 20/20 [10:42<00:00, 32.14s/it]\n",
            "100% 20/20 [09:56<00:00, 29.83s/it]\n",
            "100% 20/20 [09:45<00:00, 29.29s/it]\n",
            "100% 20/20 [10:51<00:00, 32.59s/it]\n",
            "100% 20/20 [10:05<00:00, 30.30s/it]\n",
            "100% 20/20 [10:43<00:00, 32.20s/it]\n",
            "  5% 1/20 [00:36<11:33, 36.48s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"abductive_main.py\", line 531, in <module>\n",
            "    verbose=args.verbose)\n",
            "  File \"abductive_main.py\", line 156, in generate_abductive_explanation\n",
            "    verbose=verbose\n",
            "  File \"abductive_main.py\", line 265, in delorean_decoding\n",
            "    verbose=verbose\n",
            "  File \"abductive_main.py\", line 411, in backward_pass\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 195, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJrCAdCaPJwH",
        "outputId": "4a7435f7-3d30-4031-92e7-540cb1acc47e"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/unsup_gen_for_cms_reasoning/ranking"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/unsup_gen_for_cms_reasoning/ranking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEKLnw3dTzeO"
      },
      "source": [
        "!sh 'run_counterfactual_ranking.sh'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXoHR97NVy_n"
      },
      "source": [
        "import json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjdduNFgYMNN"
      },
      "source": [
        "with open('/content/gdrive/MyDrive/unsup_gen_for_cms_reasoning/data/abductive/test-w-comet-preds.jsonl', 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "for json_str in json_list:\n",
        "    result = json.loads(json_str)\n",
        "    print(f\"result: {result}\")\n",
        "    print(isinstance(result, dict))\n",
        "\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iDZyVn4tHX_",
        "outputId": "325d6b99-f1ae-4045-b408-0faf33b0c0d6"
      },
      "source": [
        "!sh run_abductive_ranking.sh"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data size:  14313\n",
            "Downloading: 100% 436M/436M [00:11<00:00, 39.1MB/s]\n",
            "Downloading: 100% 213k/213k [00:00<00:00, 1.11MB/s]\n",
            "printing ..//output/abductive/ranking//o1o2.txt\n",
            "printing ..//output/abductive/ranking//o1ho2.txt\n",
            "printing ..//output/abductive/ranking//hypotheses.txt\n",
            "printing ..//output/abductive/ranking//ranked_hypotheses.txt\n",
            "printing ..//output/abductive/ranking//bert_score.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywyQUHAH8zdf"
      },
      "source": [
        "Huggingface alternative data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHlCV6YR4p6U",
        "outputId": "b35fe557-bb0d-48fa-bc12-f63c00ebfb1c"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('art',data_dir=\"/content/unsup_gen_for_cms_reasoning/data/abductive\" )\n",
        "dataset['train'][50000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis_1': 'It took 10 minutes for the snow plow to come through.',\n",
              " 'hypothesis_2': 'It took 10 days for the snow plow to come through.',\n",
              " 'label': 2,\n",
              " 'observation_1': 'There was ten feet of snow outside.',\n",
              " 'observation_2': 'In all that time I was unable to check my mail.'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}