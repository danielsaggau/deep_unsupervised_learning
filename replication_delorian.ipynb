{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replication_delorian.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vviEOj045PJg3mMuJk0LFmMrTYR3FXmK",
      "authorship_tag": "ABX9TyPlBwScJmKbiv9GBCq5xdKh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/deep_unsupervised_learning/blob/main/replication_delorian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o18TSMys_6lV",
        "outputId": "d1c60117-e2b1-4436-c935-9257b8f6d0f4"
      },
      "source": [
        "!git clone https://github.com/qkaren/unsup_gen_for_cms_reasoning.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'unsup_gen_for_cms_reasoning'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 106 (delta 47), reused 77 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (106/106), 3.27 MiB | 11.05 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvh5WHVUA-I5",
        "outputId": "15ba616e-7376-479f-adce-468d244da86c"
      },
      "source": [
        "%cd /content/unsup_gen_for_cms_reasoning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/unsup_gen_for_cms_reasoning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAYAKZSBWm0"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLKM0NvBTO-"
      },
      "source": [
        "!sh run_counterfactual_main.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on5ReVE_m3_q"
      },
      "source": [
        "!sh /content/unsup_gen_for_cms_reasoning/run_abductive_main.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJrCAdCaPJwH"
      },
      "source": [
        "%cd ranking/\n",
        "!sh /content/unsup_gen_for_cms_reasoning/ranking/run_counterfactual_ranking.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iDZyVn4tHX_"
      },
      "source": [
        "!sh run_abductive_ranking.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4nvM16H4vlm"
      },
      "source": [
        "\"\"\"\n",
        "DeLorean decoding for grammatical error correction \n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from operator import add\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers.modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "\n",
        "SMALL_CONST = 1e-15\n",
        "BIG_CONST = 1e10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FjnpLQfbcEz"
      },
      "source": [
        "def to_var(x, requires_grad=False, volatile=False, device=\"cuda\"):\n",
        "    if torch.cuda.is_available() and device == \"cuda\":\n",
        "        x = x.cuda()\n",
        "    elif device != \"cuda\":\n",
        "        x = x.to(device)\n",
        "    return Variable(x, requires_grad=requires_grad, volatile=volatile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec6m0kJMbeDn"
      },
      "source": [
        "def read_inputs(input_file):\n",
        "    with open(input_file) as f:\n",
        "        lines = f.readlines()\n",
        "        return [json.loads(l.strip()) for l in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXm0zo8HbgSa"
      },
      "source": [
        "def top_k_filter(logits, k, probs=False, device='cuda'):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "\n",
        "    Args:\n",
        "        probs (bool): Whether `logits` is indeed probabilities\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        if probs:\n",
        "            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n",
        "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01-UG-vP4siC"
      },
      "source": [
        "def get_input_embeds(embedding, logits , o1_onehot=None, o2_onehot=None, device='cuda'):\n",
        "    \"\"\"\n",
        "    embedding.shape = [50257, 1024]\n",
        "    \"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    if o1_onehot is not None:\n",
        "        probs = torch.cat(\n",
        "            (o1_onehot.type(torch.FloatTensor), probs.type(torch.FloatTensor)),\n",
        "            dim=1)\n",
        "    if o2_onehot is not None:\n",
        "        probs = torch.cat(\n",
        "            (probs.type(torch.FloatTensor), o2_onehot.type(torch.FloatTensor)),\n",
        "            dim=1)\n",
        "    probs = probs.to(device)\n",
        "    return torch.matmul(probs, embedding.weight)\n",
        "\n",
        "\n",
        "def get_token_from_logits(logits, temperature=1.0, top_k=1):\n",
        "    \"\"\"\n",
        "    logits.shape = [batch_size]\n",
        "    \"\"\"\n",
        "    # normalize\n",
        "    logits = top_k_filter(logits, k=top_k)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # greedy\n",
        "    _, last = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "    return last\n",
        "\n",
        "def get_text_from_logits(logits, tokenizer, temperature=1.0, top_k=1):\n",
        "    output_so_far = None\n",
        "    for i in range(logits.shape[1]):\n",
        "        last = get_token_from_logits(logits[:,i,:], temperature, top_k)\n",
        "\n",
        "        # update context/output_so_far appending the new token\n",
        "        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n",
        "\n",
        "    text = tokenizer.decode(output_so_far.tolist()[0])\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8e28OXW8jW5"
      },
      "source": [
        "def generate_grammar_correction(\n",
        "    model =None,\n",
        "    tokenizer =None,\n",
        "    device ='cuda',\n",
        "    o1_text=\"\",\n",
        "    o2_text=\"\",\n",
        "    max_length=10,\n",
        "    stepsize=0.02,\n",
        "    mix_rate=0.5,\n",
        "    temperature_forward=1.0,\n",
        "    top_k=1,\n",
        "    num_passes=3,\n",
        "    num_backward_iters=1,\n",
        "    seed=0,\n",
        "    no_cuda=False,\n",
        "    verbose=False\n",
        "): "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJ26k6_9jUW"
      },
      "source": [
        "# Set random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c4zV8MsKcTz"
      },
      "source": [
        "    # Figure out o1 o2 text\n",
        "    tokenized_o1_text = tokenizer.encode(tokenizer.bos_token + o1_text)\n",
        "    tokenized_o2_text = tokenizer.encode(o2_text + tokenizer.eos_token)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"= o1 | o2 =\")\n",
        "        print(tokenizer.decode(tokenized_o1_text))\n",
        "        print(tokenizer.decode(tokenized_o2_text))\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLzIuTVSKdkx"
      },
      "source": [
        " # Generate with DeLorean decoding\n",
        "    _, candidate_list = delorean_decoding(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        o1=tokenized_o1_text,\n",
        "        o2=tokenized_o2_text,\n",
        "        device=device,\n",
        "        length=length,\n",
        "        max_length=max_length,\n",
        "        stepsize=stepsize,\n",
        "        mix_rate=mix_rate,\n",
        "        temperature=temperature_forward,\n",
        "        top_k=top_k,\n",
        "        num_passes=num_passes,\n",
        "        num_backward_iters=num_backward_iters,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return candidate_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S14W5KueKjYH"
      },
      "source": [
        "def delorean_decoding(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    o1=None,\n",
        "    o2=None,\n",
        "    device=\"cuda\",\n",
        "    length=10,\n",
        "    max_length=20,\n",
        "    mix_rate=0.5,\n",
        "    temperature_forward=1.0,\n",
        "    top_k=1,\n",
        "    stepsize=0.02,\n",
        "    num_backward_iters=1,\n",
        "    num_passes=3,\n",
        "    verbose=False\n",
        "):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFdrcpsiK9gT"
      },
      "source": [
        "    # Prepare one-hot representations for O1 and O2\n",
        "    o1_t = torch.tensor(o1, device=device, dtype=torch.long)\n",
        "    while len(o1_t.shape) < 2:\n",
        "        o1_t = o1_t.unsqueeze(0)\n",
        "    output_so_far = o1_t\n",
        "\n",
        "    o1_onehot = torch.LongTensor(o1_t.shape[0], o1_t.shape[1], tokenizer.vocab_size)\n",
        "    o1_onehot = o1_onehot.to(device)\n",
        "    o1_onehot.zero_()\n",
        "    o1_onehot.scatter_(2, o1_t.unsqueeze(-1), 1)\n",
        "    # use a very small temperature to mimic one-hot after softmax\n",
        "    o1_logits = o1_onehot.type(torch.FloatTensor) / 0.00001\n",
        "\n",
        "    o2_t = torch.tensor(o2, device=device, dtype=torch.long)\n",
        "    while len(o2_t.shape) < 2:\n",
        "        o2_t = o2_t.unsqueeze(0)\n",
        "    \n",
        "#    o2_onehot = torch.LongTensor(o2_t.shape[0], o2_t.shape[1], tokenizer.vocab_size)\n",
        "#    o2_onehot = o2_onehot.to(device)\n",
        "#    o2_onehot.zero_()\n",
        "#    o2_onehot.scatter_(2, o2_t.unsqueeze(-1), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nppidv3Ea0vb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL81fNUxPbIw"
      },
      "source": [
        "## The initialization pass to initialize the generation (its logits)\n",
        "\n",
        "    past = None\n",
        "    last_embeds = None\n",
        "    logits_so_far = None\n",
        "    for i in range(length):\n",
        "        # run model forward to obtain unperturbed logits\n",
        "        if past is None and output_so_far is not None:\n",
        "            last = output_so_far[:, -1:]\n",
        "            last_embeds = model.get_input_embeddings()(last)\n",
        "\n",
        "            if output_so_far.shape[1] > 1:\n",
        "                _, past, _ = model(output_so_far[:, :-1])\n",
        "                o1_past = past\n",
        "\n",
        "        unpert_logits, past, unpert_all_hidden = model(past=past, inputs_embeds=last_embeds)\n",
        "        unpert_logits = unpert_logits[:, -1, :] / temperature_first\n",
        "\n",
        "        unpert_logits = unpert_logits.unsqueeze(1)\n",
        "        logits_so_far = unpert_logits if logits_so_far is None else torch.cat((logits_so_far, unpert_logits), dim=1)\n",
        "\n",
        "        last_embeds = get_input_embeds(model.get_input_embeddings(), unpert_logits / 0.01, device=device)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"[First pass]: \", get_text_from_logits(logits_so_far, tokenizer, temperature=1.0, top_k=top_k))\n",
        "\n",
        "    unpert_logits_h = logits_so_far\n",
        "\n",
        "\n",
        " ## The initialization pass to initialize the generation (its logits)\n",
        "    # Run model forward to obtain unperturbed logits\n",
        "    unpert_logits, _, _ = model(torch.cat([o1_t, o2_t], dim=-1))\n",
        "    o2_length = o2_t.shape[1]\n",
        "    o2_logits = unpert_logits[:, -o2_length-1:-1, :]  # exclude the last step which is a prediction\n",
        "    assert unpert_logits.shape[1] == o1_t.shape[1] + o2_length\n",
        "    assert o2_logits.shape[1] == o2_length\n",
        "\n",
        "    if verbose:\n",
        "        # O2 loss\n",
        "        loss = torch.nn.CrossEntropyLoss()(o2_logits.view(-1, o2_logits.size(-1)), o2_t.view(-1))\n",
        "        print(\"[First pass] recon loss: \", loss.data.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AibJbmiGa17w"
      },
      "source": [
        "## Iteratively perturb the generation through Forward and Backward passes\n",
        "\n",
        "    pert_logits = o2_logits\n",
        "\n",
        "    candidate_list = []\n",
        "    for t in trange(num_passes, ascii=True):\n",
        "\n",
        "        if verbose:\n",
        "            print()\n",
        "            print(\"=\" * 20)\n",
        "            print('Pass ', t)\n",
        "            print(\"=\" * 20)\n",
        "\n",
        "        if t > 0:\n",
        "            pert_logits = backward_pass(\n",
        "                pert_logits,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                o2=o2_t,\n",
        "                stepsize=stepsize,\n",
        "                top_k=top_k,\n",
        "                num_backward_iters=num_backward_iters,\n",
        "                device=device,\n",
        "                verbose=verbose\n",
        "            )\n",
        "\n",
        "        pert_logits, forward_text = forward_pass(\n",
        "            pert_logits,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            o1_logits=o1_logits,\n",
        "            length=o2_length,\n",
        "            max_length=o2_length + 20,\n",
        "            mix_rate=mix_rate,\n",
        "            temperature=temperature_forward,\n",
        "            top_k=top_k,\n",
        "            device=device,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        candidate_list.append(forward_text)\n",
        "\n",
        "    return output_so_far, candidate_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z72yotkGa3OK"
      },
      "source": [
        "def forward_pass(\n",
        "    logits,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    o1_logits=None,\n",
        "    length=10,\n",
        "    max_length=20,\n",
        "    mix_rate=0.5,\n",
        "    temperature=1.0,\n",
        "    top_k=1,\n",
        "    device=\"cuda\",\n",
        "    verbose=False\n",
        "):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KHYghPEa6nE"
      },
      "source": [
        "\"\"\"\n",
        "    Args:\n",
        "        length: length of the hypothesis whose logits are updated through the\n",
        "            forward-backward passes. I.e., `N` in the paper\n",
        "        max_length: we allow the forward pass to generate more than N tokens if those are\n",
        "            needed to obtain complete sentences. See section 3.1 (last paragraph) in the\n",
        "            paper. Extra tokens will be truncated.\n",
        "    \"\"\"\n",
        "    assert logits.shape[1] == length\n",
        "    h_logits = logits\n",
        "\n",
        "    past = None\n",
        "    last_embeds = None\n",
        "    logits_so_far = None\n",
        "    logits_so_far_complete = None\n",
        "    for i in range(max_length):\n",
        "        # Run model forward to obtain unperturbed logits\n",
        "        if past is None:\n",
        "            o1_embeds = get_input_embeds(model.get_input_embeddings(), o1_logits, device=device)\n",
        "            last_embeds = o1_embeds[:, -1, :].unsqueeze(1)\n",
        "\n",
        "            if o1_logits.shape[1] > 1:\n",
        "                _, past, _ = model(inputs_embeds=o1_embeds[:, :-1, :])\n",
        "\n",
        "        unpert_logits, past, unpert_all_hidden = model(past=past, inputs_embeds=last_embeds)\n",
        "        unpert_logits = unpert_logits[:, -1, :] / temperature\n",
        "\n",
        "        if i < length:\n",
        "            # Mix backward logits and forward logits, Eq.(3) in the paper\n",
        "            pert_logits = mix_rate * unpert_logits + (1-mix_rate) * h_logits[:,i,:]\n",
        "        else:\n",
        "            # Continue to complete the text\n",
        "            pert_logits = unpert_logits\n",
        "\n",
        "        pert_logits = pert_logits.unsqueeze(1)\n",
        "        if i < length:\n",
        "            logits_so_far = pert_logits if logits_so_far is None else torch.cat((logits_so_far, pert_logits), dim=1)\n",
        "        logits_so_far_complete = pert_logits if logits_so_far_complete is None else torch.cat((logits_so_far_complete, pert_logits), dim=1)\n",
        "\n",
        "        # Use a small temperature (0.1) so that the soft token representation is sharper,\n",
        "        # and closer to a one-hot representation\n",
        "        last_embeds = get_input_embeds(model.get_input_embeddings(), pert_logits / 0.1, device=device)\n",
        "\n",
        "    # Sample a text, and only extract the first sentence\n",
        "    forward_text = get_text_from_logits(logits_so_far_complete, tokenizer, temperature=1.0, top_k=top_k)\n",
        "    forward_text, _ = _extract_a_sentence(forward_text)\n",
        "    if verbose:\n",
        "        print(\"[Forward]: \", forward_text)\n",
        "\n",
        "    return logits_so_far, forward_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFD-FZO5a8To"
      },
      "source": [
        "def backward_pass(\n",
        "    logits,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    o2=None,\n",
        "    stepsize=0.01,\n",
        "    top_k=1,\n",
        "    num_backward_iters=3,\n",
        "    device=\"cuda\",\n",
        "    verbose=False\n",
        "):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siXM06YJa-L5"
      },
      "source": [
        "  # Set logits to a list just for ease of programming and experimentation\n",
        "    logits = [logits]\n",
        "\n",
        "    # Accumuated gradients w.r.t the logits\n",
        "    grad_accumulator = [(np.zeros(p.shape).astype(\"float32\")) for p in logits]\n",
        "\n",
        "    # Accumulate perturbations for num_backward_iters\n",
        "    for i in range(num_backward_iters):\n",
        "        if verbose:\n",
        "            print(\"\\n-------Iteration------- \", i + 1)\n",
        "\n",
        "        # Compute the perturbed logits\n",
        "        curr_perturbation = [\n",
        "            to_var(torch.from_numpy(p_), requires_grad=True, device=device) for p_ in grad_accumulator\n",
        "        ]\n",
        "        perturbed_logits = list(map(add, logits, curr_perturbation))\n",
        "\n",
        "        # Compute the norms of the logits for normalizing the gradients later\n",
        "        perturbed_logits_norms_all = [\n",
        "            torch.norm(p_) for index, p_ in enumerate(perturbed_logits)\n",
        "        ]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = torch.nn.CrossEntropyLoss()(\n",
        "            perturbed_logits[0].view(-1, perturbed_logits[0].size(-1)),\n",
        "            o2.view(-1))\n",
        "        if verbose:\n",
        "            print(\"loss: %.4f\" % (loss.data.cpu().numpy()))\n",
        "\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Compute gradient norms\n",
        "        grad_norms_all = [\n",
        "            (torch.norm(p_.grad) + SMALL_CONST) for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "        # Normalize and scale the gradients\n",
        "        grad = [\n",
        "            -stepsize * (p_.grad / grad_norms_all[index] * perturbed_logits_norms_all[index]).data.cpu().numpy()\n",
        "            for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "\n",
        "        # Accumulate gradients\n",
        "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
        "\n",
        "        # Reset gradients\n",
        "        for p_ in curr_perturbation:\n",
        "            p_.grad.data.zero_()\n",
        "\n",
        "        # Remove logits from the graph\n",
        "        new_logits = []\n",
        "        for p_ in logits:\n",
        "            new_logits.append(p_.detach())\n",
        "        logits = new_logits\n",
        "\n",
        "        if verbose:  # inspect the temporary text after the backward pass\n",
        "            _grad_accumulator = [to_var(torch.from_numpy(p_), requires_grad=True, device=device) for p_ in grad_accumulator]\n",
        "            _pert_logits = list(map(add, logits, _grad_accumulator))\n",
        "            text = get_text_from_logits(_pert_logits[0], tokenizer, temperature=1.0, top_k=top_k)\n",
        "            print(\"[Backward]: \", text)\n",
        "\n",
        "    # Apply the accumulated gradients to the logits\n",
        "    grad_accumulator = [to_var(torch.from_numpy(p_), requires_grad=True, device=device) for p_ in grad_accumulator]\n",
        "    pert_logits = list(map(add, logits, grad_accumulator))\n",
        "\n",
        "    return pert_logits[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qikz2WYWbt7U"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbJNAkcWbF8B"
      },
      "source": [
        "def _extract_a_sentence(text):\n",
        "    \"\"\"\n",
        "    Extracts the first sentence in `text`.\n",
        "    Returns the sentence and the remaining text.\n",
        "    \"\"\"\n",
        "    # (1)\n",
        "    sent_terminators = ['. ', '! ', '? ']\n",
        "    min_tm_index = BIG_CONST\n",
        "    for tm in sent_terminators:\n",
        "        tm_index = text.find(tm)\n",
        "        if tm_index == -1:\n",
        "            tm_index = BIG_CONST\n",
        "        min_tm_index = min(min_tm_index, tm_index)\n",
        "\n",
        "    if min_tm_index < BIG_CONST:\n",
        "        return text[:min_tm_index+1], text[min_tm_index+2:]\n",
        "\n",
        "    # (2)\n",
        "    sent_terminators = ['.\" ', '!\" ', '?\" ']\n",
        "    for tm in sent_terminators:\n",
        "        tm_index = text.find(tm)\n",
        "        if tm_index == -1:\n",
        "            tm_index = BIG_CONST\n",
        "        min_tm_index = min(min_tm_index, tm_index)\n",
        "\n",
        "    if min_tm_index < BIG_CONST:\n",
        "        return text[:min_tm_index+2], text[min_tm_index+3:]\n",
        "\n",
        "    return text, \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO0hME5nh-fl"
      },
      "source": [
        "def extract_three_sentences(text):\n",
        "    \"\"\"\n",
        "    `text` is assumed to consist of three sentences. This function\n",
        "    extracts and returns the three sentences.\n",
        "    \"\"\"\n",
        "    s1, s23 = _extract_a_sentence(text)\n",
        "    s2, s3 = _extract_a_sentence(s23)\n",
        "    return s1, s2, s3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5PCaDBObtPH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfPYzOhJbKEF"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--pretrained_model\", type=str, default=\"gpt2-medium\",\n",
        "        help=\"pretrained model name or path to local checkpoint\")\n",
        "    parser.add_argument(\n",
        "        \"--length\", type=int, default=10,\n",
        "        help=\"Length of generated text. Not used in the counterfactual setting because the generation length \"\n",
        "             \"is set to the length of the original story ending.\")\n",
        "    parser.add_argument(\n",
        "        \"--max_length\", type=int, default=20,\n",
        "        help=\"Max length of generated text. We allow the forward pass to generate more than `length` tokens if \"\n",
        "             \"those are needed to obtain complete sentences. See section 3.1 (last paragraph) for details.\")\n",
        "    parser.add_argument(\"--mix_rate\", type=float, default=0.5, help=\"Weight of mixing backward and forward logits in the forward pass.\")\n",
        "    parser.add_argument(\"--temperature_forward\", type=float, default=1.0, help=\"Temperature of logits used in the forward pass.\")\n",
        "    parser.add_argument(\"--top_k\", type=int, default=1, help=\"Top-k sampling from logits.\")\n",
        "    parser.add_argument(\"--stepsize\", type=float, default=0.02, help=\"learning rate in the backward pass.\")\n",
        "    parser.add_argument(\"--num_backward_iters\", type=int, default=1, help=\"Number of backpropagation iterations in a Backward pass.\")\n",
        "    parser.add_argument(\"--num_passes\", type=int, default=3, help=\"Number of passes to interleave Forward and Backward.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed.\")\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"no cuda\")\n",
        "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Print intermediate states to help with tuning / debugging.\")\n",
        "    parser.add_argument(\"--input_file\", type=str, default=\"\", help=\"Input data in json format.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"\", help=\"Output dir.\")\n",
        "\n",
        "    args = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pRSff4YRkd8"
      },
      "source": [
        "# Set the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "\n",
        "    # Load pretrained model\n",
        "    model = GPT2LMHeadModel.from_pretrained(args.pretrained_model, output_hidden_states=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    # Load tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(args.pretrained_model)\n",
        "    # Freeze GPT-2 weights\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    candidate_output = './{}/counterfactual_output_np{}_nbi{}.json'.format(\n",
        "        args.output_dir, args.num_passes, args.num_backward_iters)\n",
        "\n",
        "    records = read_inputs(args.input_file)\n",
        "\n",
        "    procssed = set()\n",
        "    candidate_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oteRMWl8bLg0"
      },
      "source": [
        "# `fw` outputs all results, `fw_text` outputs the cleaned results\n",
        "    with open(candidate_output, 'w') as fw, open(candidate_output+'.txt', 'w') as fw_txt:\n",
        "        for r in records:\n",
        "            o1_text = ' '.join([r['premise'], r['counterfactual']])\n",
        "            o2_text = r['original_ending']\n",
        "\n",
        "            # The original dataset can include repeated instances.\n",
        "            # We keep track and skip instances that are already processed\n",
        "            if o1_text in procssed:\n",
        "                continue\n",
        "            else:\n",
        "                procssed.add(o1_text)\n",
        "\n",
        "            # o2_text has three sentences. We use DoLorean to generate one\n",
        "            # sentence at a time. See Appendix A.2 in the paper for more details.\n",
        "            o2_text_sents = extract_three_sentences(o2_text)\n",
        "\n",
        "            o2_text_so_far = \"\"\n",
        "            o1_text_so_far = \"\"\n",
        "            o1_addon = o1_text\n",
        "\n",
        "            for o2_sent in o2_text_sents:\n",
        "                o1_text_so_far = o1_text_so_far.strip() + \" \" + o1_addon.strip()\n",
        "                o2_text_so_far = o2_sent.strip()\n",
        "\n",
        "                # We want to ensure a space token between o1_text and o2_text\n",
        "                # during the decoding. To do so, here we append \". \" so that\n",
        "                # the GPT2 tokenizer later will not strip the space token. After\n",
        "                # tokenization, we delete the \".\" token.\n",
        "                o2_text_so_far = \". \" + o2_text_so_far\n",
        "\n",
        "                candidate_list = generate_counterfactual_story_endings(\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device,\n",
        "                    o1_text=o1_text_so_far,\n",
        "                    o2_text=o2_text_so_far,\n",
        "                    max_length=args.max_length,\n",
        "                    stepsize=args.stepsize,\n",
        "                    mix_rate=args.mix_rate,\n",
        "                    temperature_forward=args.temperature_forward,\n",
        "                    top_k=args.top_k,\n",
        "                    num_passes=args.num_passes,\n",
        "                    num_backward_iters=args.num_backward_iters,\n",
        "                    seed=args.seed,\n",
        "                    no_cuda=args.no_cuda,\n",
        "                    verbose=args.verbose)\n",
        "\n",
        "                d = {\n",
        "                    'premise': r['premise'],\n",
        "                    'initial': r['initial'],\n",
        "                    'counterfactual': r['counterfactual'],\n",
        "                    'original_ending': o2_text,\n",
        "                    'counterfactual_so_far': o1_text_so_far,\n",
        "                    'original_ending_so_far': o2_text_so_far,\n",
        "                    'H_Candidates': candidate_list\n",
        "                }\n",
        "                fw.write(json.dumps(d) + '\\n')\n",
        "                fw_txt.write(candidate_list[-1] + '\\n')\n",
        "\n",
        "                o1_addon = candidate_list[-1]  # pick the last candidate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}