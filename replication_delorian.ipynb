{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replication_delorian.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vviEOj045PJg3mMuJk0LFmMrTYR3FXmK",
      "authorship_tag": "ABX9TyNqHpgRncLRwcmn4qGLUkaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/deep_unsupervised_learning/blob/main/replication_delorian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o18TSMys_6lV",
        "outputId": "c725e8ba-c4c3-4e97-e4e8-d2cfefee8e8f"
      },
      "source": [
        "!git clone 'https://github.com/danielsaggau/unsup_gen_for_cms_reasoning.git'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'unsup_gen_for_cms_reasoning'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 121 (delta 50), reused 76 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (121/121), 5.36 MiB | 8.56 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvh5WHVUA-I5",
        "outputId": "e2bd781b-25d1-4245-d810-19fbcf4ae895"
      },
      "source": [
        "%cd /content/unsup_gen_for_cms_reasoning"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/unsup_gen_for_cms_reasoning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_7mHWBFP5Zq"
      },
      "source": [
        "import json\n",
        "data = [json.loads(line) for line in open('/content/unsup_gen_for_cms_reasoning/data/C.train.json', 'r')]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbie0cwuQser",
        "outputId": "6dbb59d7-6878-4816-a6b8-88789b199058"
      },
      "source": [
        "data[1:5]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'cefr': 'C2.ii',\n",
              "  'edits': [[0,\n",
              "    [[598, 617, 'a glass next to him'],\n",
              "     [617, 618, ''],\n",
              "     [684, 687, ''],\n",
              "     [814, 814, ','],\n",
              "     [869, 869, ','],\n",
              "     [902, 904, ','],\n",
              "     [916, 916, ','],\n",
              "     [1266, 1266, ','],\n",
              "     [1380, 1380, ','],\n",
              "     [1419, 1419, '.']]]],\n",
              "  'id': '1-65920',\n",
              "  'text': \"I was walking in the center of Turin when I saw a big, beautiful hotel, with a lot of police who were trying to maintain law and order because there were a lot of people like me that wanted to know what was going on.\\n\\nI saw a policeman who was my friend and he explained to me that a man had been killed by someone and he allowed me to go into the room and see the scene of the crime.\\n\\nI went to see the room of this poor man, called Mr Smith. All the room was in order and the policeman explained to me that the man, who had been very old, had also been rich. Mr Smith was found on the floor with next to him a glass, which was completely broken.\\n\\nI thought that he must have called the reception for some wine, as the waitress said that every night after he had eaten, he always drank a glass of wine. \\nAll night the body had lain like this on the floor. The next day the waitress knocked once, twice , three times but no one answered, so she thought that the man might have gone out. She opened the room with some keys that all the waitresses have, and saw Mr Smith lying dead on the floor. The detectives analysed what was in the glass and it was poison. \\n\\nI think that someone who wanted to inherit Mr Smithâ€™s money must have killed him for this reason. In fact I discovered that Mr Smith's nephew wanted all his fortune. So he must have killed him with the poison. Sometimes people will do anything for some money\\n\",\n",
              "  'userid': '8135'},\n",
              " {'cefr': 'C1.i',\n",
              "  'edits': [[0,\n",
              "    [[9, 9, ','],\n",
              "     [86, 91, 'At first,'],\n",
              "     [105, 110, 'agreed'],\n",
              "     [120, 120, ','],\n",
              "     [209, 209, ','],\n",
              "     [244, 244, ','],\n",
              "     [325, 325, ' with'],\n",
              "     [330, 335, 'can'],\n",
              "     [359, 369, 'complete'],\n",
              "     [376, 376, ','],\n",
              "     [469, 469, ','],\n",
              "     [534, 541, 'companionship'],\n",
              "     [576, 581, 'begin'],\n",
              "     [677, 677, ','],\n",
              "     [706, 706, ','],\n",
              "     [713, 720, 'more fun'],\n",
              "     [853, 857, 'the team'],\n",
              "     [873, 880, 'companionship'],\n",
              "     [889, 894, 'can'],\n",
              "     [895, 899, 'have'],\n",
              "     [916, 916, '.']]]],\n",
              "  'id': '1-302620',\n",
              "  'text': \" Nowadays it's believed that it's better to work on your own than to work as a group. First I completely agree with this but then when I started to think about it I realized that it's not like that. \\n\\n Firstly I think that if you work as a team  you can help each other and if you have more people to talk about any situation you could have a better and more completely answer because you have the ideas and the knowledge of every single member of the group.\\n \\nSecondly I think that the members of the team give you support, love and company. Sometimes the members of a group began to be so close to you that you love them the same as if they were part of your family. Moreover if it is a really nice group it is funnier to work with them than to work alone.\\n\\nIn conclusion, I believe that it's better to work as a team than to work on your own because they give you love, company and you could pass a very good time\",\n",
              "  'userid': '38406'},\n",
              " {'cefr': 'C1.i',\n",
              "  'edits': [[0,\n",
              "    [[62, 62, ','],\n",
              "     [208, 216, 'choice'],\n",
              "     [217, 222, 'may'],\n",
              "     [240, 240, ','],\n",
              "     [425, 428, ''],\n",
              "     [560, 569, 'spread'],\n",
              "     [601, 607, 'led'],\n",
              "     [608, 610, 'to'],\n",
              "     [776, 776, ','],\n",
              "     [819, 819, ','],\n",
              "     [838, 840, 'like'],\n",
              "     [906, 906, ','],\n",
              "     [948, 950, 'for'],\n",
              "     [1015, 1023, 'recent'],\n",
              "     [1029, 1029, ','],\n",
              "     [1101, 1105, 'hand'],\n",
              "     [1111, 1117, 'become'],\n",
              "     [1139, 1139, ','],\n",
              "     [1166, 1166, ','],\n",
              "     [1188, 1188, ','],\n",
              "     [1199, 1199, 'it '],\n",
              "     [1331, 1333, 'for'],\n",
              "     [1334, 1339, 'watching'],\n",
              "     [1389, 1389, ','],\n",
              "     [1423, 1426, ''],\n",
              "     [1454, 1456, 'to']]]],\n",
              "  'id': '1-32079',\n",
              "  'text': \"It is well known that an image is better than a thousand words and that may be the reason for the success of television in a recent survey as the most important invention of the last 100 years.\\nHowever, this election could not be the wisest as there are other inventions more useful than television and with better future applications.\\nI am referring to the Internet. Born in the 1970's as a military project to interconnect the missile facilities and the central headquarters, it soon spread to universities and educational centers. This fact, along with the spreading of personal computers, quickly became in a generalization of its use in the first decade of the XXI century.\\nNobody wants to dismiss the importance of television and its achievements. For more than 50 years it has been the main entertainment channel displacing others as radio, theater, cinema and, sadly, also books.\\nIn its first years television was also an important channel to culture (theater plays, classic films, etc.). Unfortunately, in the last years television has lost this educational focus.\\nThe Internet, on the other side, has became more and more popular not only for entertainment but also for business and today is almost impossible to find an activity in which the Internet doesn't play a major role.\\nThe Internet has also replaced television to watch films and TV shows, especially among young people who prefer to download and watch the videos instead of watching on the fixed schedule of TV.\",\n",
              "  'userid': '1903'},\n",
              " {'cefr': 'C1.ii',\n",
              "  'edits': [[0,\n",
              "    [[77, 79, 'for'],\n",
              "     [125, 127, 'I'],\n",
              "     [307, 309, 'I'],\n",
              "     [351, 361, \"day's work,\"],\n",
              "     [485, 485, ','],\n",
              "     [658, 660, 'I'],\n",
              "     [751, 754, 'that'],\n",
              "     [822, 824, 'came'],\n",
              "     [888, 891, 'but'],\n",
              "     [912, 917, 'sounds']]]],\n",
              "  'id': '1-173395',\n",
              "  'text': \"A fire in the French Alps\\nThis experience is  unbelievable and unforgettable to me. It was in winter 2002 when 2 friends and me were working in a hotel in a village called Tignes. The hotel had 8 floors and the workers lived on the top floor.\\n\\tAt about 8 p.m in the evening on December 12th, my friends and me went back home. It was the end of a hard work's day so we went to bed early; we were very tired and we had forgotten to put out a candle. Then, disaster struck. One hour later our table was on fire. \\n\\tWhile the fire was spreading all over the bedroom, I woke up and Matxi was unconscious because there was a lot of smoke in the bedroom. Monica and me called out her name and we slapped her face. Fortunately, we managed to wake her up. \\n\\tAt the moment, some friends and other workers who lived on the same floor go into the bedroom. At the beginning, we couldn't stop the fire, and although this night sound impossible, we managed to put out the fire. \\n\\tThat day I was afraid, but now I remember this experience as a good time.  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
              "  'userid': '25200'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZAYAKZSBWm0"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLLKM0NvBTO-"
      },
      "source": [
        "!sh run_counterfactual_main.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on5ReVE_m3_q"
      },
      "source": [
        "!sh /content/unsup_gen_for_cms_reasoning/run_abductive_main.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJrCAdCaPJwH"
      },
      "source": [
        "%cd ranking/\n",
        "!sh /content/unsup_gen_for_cms_reasoning/ranking/run_counterfactual_ranking.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iDZyVn4tHX_"
      },
      "source": [
        "!sh run_abductive_ranking.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4nvM16H4vlm"
      },
      "source": [
        "\"\"\"\n",
        "DeLorean decoding for grammatical error correction \n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "from operator import add\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers.modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "\n",
        "SMALL_CONST = 1e-15\n",
        "BIG_CONST = 1e10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FjnpLQfbcEz"
      },
      "source": [
        "def to_var(x, requires_grad=False, volatile=False, device=\"cuda\"):\n",
        "    if torch.cuda.is_available() and device == \"cuda\":\n",
        "        x = x.cuda()\n",
        "    elif device != \"cuda\":\n",
        "        x = x.to(device)\n",
        "    return Variable(x, requires_grad=requires_grad, volatile=volatile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec6m0kJMbeDn"
      },
      "source": [
        "def read_inputs(input_file):\n",
        "    with open(input_file) as f:\n",
        "        lines = f.readlines()\n",
        "        return [json.loads(l.strip()) for l in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXm0zo8HbgSa"
      },
      "source": [
        "def top_k_filter(logits, k, probs=False, device='cuda'):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "\n",
        "    Args:\n",
        "        probs (bool): Whether `logits` is indeed probabilities\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        if probs:\n",
        "            return torch.where(logits < batch_mins, torch.ones_like(logits) * 0.0, logits)\n",
        "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -BIG_CONST, logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01-UG-vP4siC"
      },
      "source": [
        "def get_input_embeds(embedding, logits , o1_onehot=None, o2_onehot=None, device='cuda'):\n",
        "    \"\"\"\n",
        "    embedding.shape = [50257, 1024]\n",
        "    \"\"\"\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    if o1_onehot is not None:\n",
        "        probs = torch.cat(\n",
        "            (o1_onehot.type(torch.FloatTensor), probs.type(torch.FloatTensor)),\n",
        "            dim=1)\n",
        "    if o2_onehot is not None:\n",
        "        probs = torch.cat(\n",
        "            (probs.type(torch.FloatTensor), o2_onehot.type(torch.FloatTensor)),\n",
        "            dim=1)\n",
        "    probs = probs.to(device)\n",
        "    return torch.matmul(probs, embedding.weight)\n",
        "\n",
        "\n",
        "def get_token_from_logits(logits, temperature=1.0, top_k=1):\n",
        "    \"\"\"\n",
        "    logits.shape = [batch_size]\n",
        "    \"\"\"\n",
        "    # normalize\n",
        "    logits = top_k_filter(logits, k=top_k)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    # greedy\n",
        "    _, last = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "    return last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReWemSEdBVG"
      },
      "source": [
        "def get_text_from_logits(logits, tokenizer, temperature=1.0, top_k=1):\n",
        "    output_so_far = None\n",
        "    for i in range(logits.shape[1]):\n",
        "        last = get_token_from_logits(logits[:,i,:], temperature, top_k)\n",
        "\n",
        "        # update context/output_so_far appending the new token\n",
        "        output_so_far = last if output_so_far is None else torch.cat((output_so_far, last), dim=1)\n",
        "\n",
        "    text = tokenizer.decode(output_so_far.tolist()[0])\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8e28OXW8jW5"
      },
      "source": [
        "def generate_grammar_correction(\n",
        "    model =None,\n",
        "    tokenizer =None,\n",
        "    device ='cuda',\n",
        "    o1_text=\"\",\n",
        "    o2_text=\"\",\n",
        "    max_length=10,\n",
        "    stepsize=0.02,\n",
        "    mix_rate=0.5,\n",
        "    temperature_forward=1.0,\n",
        "    top_k=1,\n",
        "    num_passes=3,\n",
        "    num_backward_iters=1,\n",
        "    seed=0,\n",
        "    no_cuda=False,\n",
        "    verbose=False\n",
        "): "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrXNw14ZO8Pj"
      },
      "source": [
        "keep random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJ26k6_9jUW"
      },
      "source": [
        "# Set random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeW_pvtKO6G5"
      },
      "source": [
        "need to modify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c4zV8MsKcTz"
      },
      "source": [
        "    # Figure out o1 o2 text\n",
        "    tokenized_o1_text = tokenizer.encode(tokenizer.bos_token + o1_text)\n",
        "    tokenized_o2_text = tokenizer.encode(o2_text + tokenizer.eos_token)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"= o1 | o2 =\")\n",
        "        print(tokenizer.decode(tokenized_o1_text))\n",
        "        print(tokenizer.decode(tokenized_o2_text))\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLzIuTVSKdkx"
      },
      "source": [
        " # Generate with DeLorean decoding\n",
        "    _, candidate_list = delorean_decoding(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        o1=tokenized_o1_text,\n",
        "        o2=tokenized_o2_text,\n",
        "        device=device,\n",
        "        length=length,\n",
        "        max_length=max_length,\n",
        "        stepsize=stepsize,\n",
        "        mix_rate=mix_rate,\n",
        "        temperature=temperature_forward,\n",
        "        top_k=top_k,\n",
        "        num_passes=num_passes,\n",
        "        num_backward_iters=num_backward_iters,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return candidate_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S14W5KueKjYH"
      },
      "source": [
        "def delorean_decoding(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    o1=None,\n",
        "    o2=None,\n",
        "    device=\"cuda\",\n",
        "    length=10,\n",
        "    max_length=20, # max length differs in implemenation\n",
        "    mix_rate=0.5,\n",
        "    temperature_forward=1.0, # difference temp forward and backward and normal\n",
        "    top_k=1,\n",
        "    stepsize=0.02,\n",
        "    num_backward_iters=1,\n",
        "    num_passes=3,\n",
        "    verbose=False\n",
        "):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFdrcpsiK9gT"
      },
      "source": [
        "    # Prepare one-hot representations for O1 and O2\n",
        "    o1_t = torch.tensor(o1, device=device, dtype=torch.long)\n",
        "    while len(o1_t.shape) < 2:\n",
        "        o1_t = o1_t.unsqueeze(0)\n",
        "    output_so_far = o1_t\n",
        "\n",
        "    o1_onehot = torch.LongTensor(o1_t.shape[0], o1_t.shape[1], tokenizer.vocab_size)\n",
        "    o1_onehot = o1_onehot.to(device)\n",
        "    o1_onehot.zero_()\n",
        "    o1_onehot.scatter_(2, o1_t.unsqueeze(-1), 1)\n",
        "    # use a very small temperature to mimic one-hot after softmax\n",
        "    o1_logits = o1_onehot.type(torch.FloatTensor) / 0.00001\n",
        "\n",
        "    o2_t = torch.tensor(o2, device=device, dtype=torch.long)\n",
        "    while len(o2_t.shape) < 2:\n",
        "        o2_t = o2_t.unsqueeze(0)\n",
        "    \n",
        "#    o2_onehot = torch.LongTensor(o2_t.shape[0], o2_t.shape[1], tokenizer.vocab_size)\n",
        "#    o2_onehot = o2_onehot.to(device)\n",
        "#    o2_onehot.zero_()\n",
        "#    o2_onehot.scatter_(2, o2_t.unsqueeze(-1), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nppidv3Ea0vb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL81fNUxPbIw"
      },
      "source": [
        "## The initialization pass to initialize the generation (its logits)\n",
        "\n",
        "    past = None\n",
        "    last_embeds = None\n",
        "    logits_so_far = None\n",
        "    for i in range(length):\n",
        "        # run model forward to obtain unperturbed logits\n",
        "        if past is None and output_so_far is not None:\n",
        "            last = output_so_far[:, -1:]\n",
        "            last_embeds = model.get_input_embeddings()(last)\n",
        "\n",
        "            if output_so_far.shape[1] > 1:\n",
        "                _, past, _ = model(output_so_far[:, :-1])\n",
        "                o1_past = past\n",
        "\n",
        "        unpert_logits, past, unpert_all_hidden = model(past=past, inputs_embeds=last_embeds)\n",
        "        unpert_logits = unpert_logits[:, -1, :] / temperature_first\n",
        "\n",
        "        unpert_logits = unpert_logits.unsqueeze(1)\n",
        "        logits_so_far = unpert_logits if logits_so_far is None else torch.cat((logits_so_far, unpert_logits), dim=1)\n",
        "\n",
        "        last_embeds = get_input_embeds(model.get_input_embeddings(), unpert_logits / 0.01, device=device)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"[First pass]: \", get_text_from_logits(logits_so_far, tokenizer, temperature=1.0, top_k=top_k))\n",
        "\n",
        "    unpert_logits_h = logits_so_far"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrA3l_17c-Kd"
      },
      "source": [
        " ## The initialization pass to initialize the generation (its logits)\n",
        "    # Run model forward to obtain unperturbed logits\n",
        "    unpert_logits, _, _ = model(torch.cat([o1_t, o2_t], dim=-1))\n",
        "    o2_length = o2_t.shape[1]\n",
        "    o2_logits = unpert_logits[:, -o2_length-1:-1, :]  # exclude the last step which is a prediction\n",
        "    assert unpert_logits.shape[1] == o1_t.shape[1] + o2_length\n",
        "    assert o2_logits.shape[1] == o2_length\n",
        "\n",
        "    if verbose:\n",
        "        # O2 loss\n",
        "        loss = torch.nn.CrossEntropyLoss()(o2_logits.view(-1, o2_logits.size(-1)), o2_t.view(-1))\n",
        "        print(\"[First pass] recon loss: \", loss.data.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AibJbmiGa17w"
      },
      "source": [
        "## Iteratively perturb the generation through Forward and Backward passes\n",
        "\n",
        "    pert_logits = o2_logits\n",
        "\n",
        "    candidate_list = []\n",
        "    for t in trange(num_passes, ascii=True):\n",
        "\n",
        "        if verbose:\n",
        "            print()\n",
        "            print(\"=\" * 20)\n",
        "            print('Pass ', t)\n",
        "            print(\"=\" * 20)\n",
        "\n",
        "        if t > 0:\n",
        "            pert_logits = backward_pass(\n",
        "                pert_logits,\n",
        "                model,\n",
        "                tokenizer,\n",
        "                o2=o2_t,\n",
        "                stepsize=stepsize,\n",
        "                top_k=top_k,\n",
        "                num_backward_iters=num_backward_iters,\n",
        "                device=device,\n",
        "                verbose=verbose\n",
        "            )\n",
        "\n",
        "        pert_logits, forward_text = forward_pass(\n",
        "            pert_logits,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            o1_logits=o1_logits,\n",
        "            length=o2_length,\n",
        "            max_length=o2_length + 20,\n",
        "            mix_rate=mix_rate,\n",
        "            temperature=temperature_forward,\n",
        "            top_k=top_k,\n",
        "            device=device,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        candidate_list.append(forward_text)\n",
        "\n",
        "    return output_so_far, candidate_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z72yotkGa3OK"
      },
      "source": [
        "def forward_pass(\n",
        "    logits,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    o1_logits=None,\n",
        "    length=10,\n",
        "    max_length=20,\n",
        "    mix_rate=0.5,\n",
        "    temperature=1.0, # why temp here 1 \n",
        "    top_k=1,\n",
        "    device=\"cuda\",\n",
        "    verbose=False\n",
        "):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KHYghPEa6nE"
      },
      "source": [
        "\"\"\"\n",
        "    Args:\n",
        "        length: length of the hypothesis whose logits are updated through the\n",
        "            forward-backward passes. I.e., `N` in the paper\n",
        "        max_length: we allow the forward pass to generate more than N tokens if those are\n",
        "            needed to obtain complete sentences. See section 3.1 (last paragraph) in the\n",
        "            paper. Extra tokens will be truncated.\n",
        "    \"\"\"\n",
        "    assert logits.shape[1] == length\n",
        "    h_logits = logits\n",
        "\n",
        "    past = None\n",
        "    last_embeds = None\n",
        "    logits_so_far = None\n",
        "    logits_so_far_complete = None\n",
        "    for i in range(max_length):\n",
        "        # Run model forward to obtain unperturbed logits\n",
        "        if past is None:\n",
        "            o1_embeds = get_input_embeds(model.get_input_embeddings(), o1_logits, device=device)\n",
        "            last_embeds = o1_embeds[:, -1, :].unsqueeze(1)\n",
        "\n",
        "            if o1_logits.shape[1] > 1:\n",
        "                _, past, _ = model(inputs_embeds=o1_embeds[:, :-1, :])\n",
        "\n",
        "        unpert_logits, past, unpert_all_hidden = model(past=past, inputs_embeds=last_embeds)\n",
        "        unpert_logits = unpert_logits[:, -1, :] / temperature\n",
        "\n",
        "        if i < length:\n",
        "            # Mix backward logits and forward logits, Eq.(3) in the paper\n",
        "            pert_logits = mix_rate * unpert_logits + (1-mix_rate) * h_logits[:,i,:]\n",
        "        else:\n",
        "            # Continue to complete the text\n",
        "            pert_logits = unpert_logits\n",
        "\n",
        "        pert_logits = pert_logits.unsqueeze(1)\n",
        "        if i < length:\n",
        "            logits_so_far = pert_logits if logits_so_far is None else torch.cat((logits_so_far, pert_logits), dim=1)\n",
        "        logits_so_far_complete = pert_logits if logits_so_far_complete is None else torch.cat((logits_so_far_complete, pert_logits), dim=1)\n",
        "\n",
        "        # Use a small temperature (0.1) so that the soft token representation is sharper,\n",
        "        # and closer to a one-hot representation\n",
        "        last_embeds = get_input_embeds(model.get_input_embeddings(), pert_logits / 0.1, device=device)\n",
        "\n",
        "    # Sample a text, and only extract the first sentence\n",
        "    forward_text = get_text_from_logits(logits_so_far_complete, tokenizer, temperature=1.0, top_k=top_k)\n",
        "    forward_text, _ = _extract_a_sentence(forward_text)\n",
        "    if verbose:\n",
        "        print(\"[Forward]: \", forward_text)\n",
        "\n",
        "    return logits_so_far, forward_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFD-FZO5a8To"
      },
      "source": [
        "def backward_pass(\n",
        "    logits,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    o2=None,\n",
        "    stepsize=0.01,\n",
        "    top_k=1,\n",
        "    num_backward_iters=3,\n",
        "    device=\"cuda\",\n",
        "    verbose=False\n",
        "):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siXM06YJa-L5"
      },
      "source": [
        "  # Set logits to a list just for ease of programming and experimentation\n",
        "    logits = [logits]\n",
        "\n",
        "    # Accumulated gradients w.r.t the logits\n",
        "    grad_accumulator = [(np.zeros(p.shape).astype(\"float32\")) for p in logits]\n",
        "\n",
        "    # Accumulate perturbations for num_backward_iters\n",
        "    for i in range(num_backward_iters):\n",
        "        if verbose:\n",
        "            print(\"\\n-------Iteration------- \", i + 1)\n",
        "\n",
        "        # Compute the perturbed logits\n",
        "        curr_perturbation = [\n",
        "            to_var(torch.from_numpy(p_), requires_grad=True, device=device) for p_ in grad_accumulator\n",
        "        ]\n",
        "        perturbed_logits = list(map(add, logits, curr_perturbation))\n",
        "\n",
        "        # Compute the norms of the logits for normalizing the gradients later\n",
        "        perturbed_logits_norms_all = [\n",
        "            torch.norm(p_) for index, p_ in enumerate(perturbed_logits)\n",
        "        ]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = torch.nn.CrossEntropyLoss()(\n",
        "            perturbed_logits[0].view(-1, perturbed_logits[0].size(-1)),\n",
        "            o2.view(-1))\n",
        "        if verbose:\n",
        "            print(\"loss: %.4f\" % (loss.data.cpu().numpy()))\n",
        "\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Compute gradient norms\n",
        "        grad_norms_all = [\n",
        "            (torch.norm(p_.grad) + SMALL_CONST) for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "        # Normalize and scale the gradients\n",
        "        grad = [\n",
        "            -stepsize * (p_.grad / grad_norms_all[index] * perturbed_logits_norms_all[index]).data.cpu().numpy()\n",
        "            for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "\n",
        "        # Accumulate gradients\n",
        "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
        "\n",
        "        # Reset gradients\n",
        "        for p_ in curr_perturbation:\n",
        "            p_.grad.data.zero_()\n",
        "\n",
        "        # Remove logits from the graph\n",
        "        new_logits = []\n",
        "        for p_ in logits:\n",
        "            new_logits.append(p_.detach())\n",
        "        logits = new_logits\n",
        "\n",
        "        if verbose:  # inspect the temporary text after the backward pass\n",
        "            _grad_accumulator = [to_var(torch.from_numpy(p_), requires_grad=True, device=device) for p_ in grad_accumulator]\n",
        "            _pert_logits = list(map(add, logits, _grad_accumulator))\n",
        "            text = get_text_from_logits(_pert_logits[0], tokenizer, temperature=1.0, top_k=top_k)\n",
        "            print(\"[Backward]: \", text)\n",
        "\n",
        "    # Apply the accumulated gradients to the logits\n",
        "    grad_accumulator = [to_var(torch.from_numpy(p_), requires_grad=True, device=device) for p_ in grad_accumulator]\n",
        "    pert_logits = list(map(add, logits, grad_accumulator))\n",
        "\n",
        "    return pert_logits[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qikz2WYWbt7U"
      },
      "source": [
        "From Counterfactual example; might omit "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbJNAkcWbF8B"
      },
      "source": [
        "def _extract_a_sentence(text):\n",
        "    \"\"\"\n",
        "    Extracts the first sentence in `text`.\n",
        "    Returns the sentence and the remaining text.\n",
        "    \"\"\"\n",
        "    # (1)\n",
        "    sent_terminators = ['. ', '! ', '? ']\n",
        "    min_tm_index = BIG_CONST\n",
        "    for tm in sent_terminators:\n",
        "        tm_index = text.find(tm)\n",
        "        if tm_index == -1:\n",
        "            tm_index = BIG_CONST\n",
        "        min_tm_index = min(min_tm_index, tm_index)\n",
        "\n",
        "    if min_tm_index < BIG_CONST:\n",
        "        return text[:min_tm_index+1], text[min_tm_index+2:]\n",
        "\n",
        "    # (2)\n",
        "    sent_terminators = ['.\" ', '!\" ', '?\" ']\n",
        "    for tm in sent_terminators:\n",
        "        tm_index = text.find(tm)\n",
        "        if tm_index == -1:\n",
        "            tm_index = BIG_CONST\n",
        "        min_tm_index = min(min_tm_index, tm_index)\n",
        "\n",
        "    if min_tm_index < BIG_CONST:\n",
        "        return text[:min_tm_index+2], text[min_tm_index+3:]\n",
        "\n",
        "    return text, \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO0hME5nh-fl"
      },
      "source": [
        "def extract_three_sentences(text):\n",
        "    \"\"\"\n",
        "    `text` is assumed to consist of three sentences. This function\n",
        "    extracts and returns the three sentences.\n",
        "    \"\"\"\n",
        "    s1, s23 = _extract_a_sentence(text)\n",
        "    s2, s3 = _extract_a_sentence(s23)\n",
        "    return s1, s2, s3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5PCaDBObtPH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfPYzOhJbKEF"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--pretrained_model\", type=str, default=\"gpt2-medium\",\n",
        "        help=\"pretrained model name or path to local checkpoint\")\n",
        "    parser.add_argument(\n",
        "        \"--length\", type=int, default=10,\n",
        "        help=\"Length of generated text. Not used in the counterfactual setting because the generation length \"\n",
        "             \"is set to the length of the original story ending.\")\n",
        "    parser.add_argument(\n",
        "        \"--max_length\", type=int, default=20,\n",
        "        help=\"Max length of generated text. We allow the forward pass to generate more than `length` tokens if \"\n",
        "             \"those are needed to obtain complete sentences. See section 3.1 (last paragraph) for details.\")\n",
        "    parser.add_argument(\"--mix_rate\", type=float, default=0.5, help=\"Weight of mixing backward and forward logits in the forward pass.\")\n",
        "    parser.add_argument(\"--temperature_forward\", type=float, default=1.0, help=\"Temperature of logits used in the forward pass.\")\n",
        "    parser.add_argument(\"--top_k\", type=int, default=1, help=\"Top-k sampling from logits.\")\n",
        "    parser.add_argument(\"--stepsize\", type=float, default=0.02, help=\"learning rate in the backward pass.\")\n",
        "    parser.add_argument(\"--num_backward_iters\", type=int, default=1, help=\"Number of backpropagation iterations in a Backward pass.\")\n",
        "    parser.add_argument(\"--num_passes\", type=int, default=3, help=\"Number of passes to interleave Forward and Backward.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed.\")\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"no cuda\")\n",
        "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Print intermediate states to help with tuning / debugging.\")\n",
        "    parser.add_argument(\"--input_file\", type=str, default=\"\", help=\"Input data in json format.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"\", help=\"Output dir.\")\n",
        "\n",
        "    args = parser.parse_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pRSff4YRkd8"
      },
      "source": [
        "# Set the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "\n",
        "    # Load pretrained model\n",
        "    model = GPT2LMHeadModel.from_pretrained(args.pretrained_model, output_hidden_states=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    # Load tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(args.pretrained_model)\n",
        "    # Freeze GPT-2 weights\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    candidate_output = './{}/counterfactual_output_np{}_nbi{}.json'.format(\n",
        "        args.output_dir, args.num_passes, args.num_backward_iters)\n",
        "\n",
        "    records = read_inputs(args.input_file)\n",
        "\n",
        "    procssed = set()\n",
        "    candidate_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oteRMWl8bLg0"
      },
      "source": [
        "# `fw` outputs all results, `fw_text` outputs the cleaned results\n",
        "    with open(candidate_output, 'w') as fw, open(candidate_output+'.txt', 'w') as fw_txt:\n",
        "        for r in records:\n",
        "            o1_text = ' '.join([r['premise'], r['counterfactual']])\n",
        "            o2_text = r['original_ending']\n",
        "\n",
        "            # The original dataset can include repeated instances.\n",
        "            # We keep track and skip instances that are already processed\n",
        "            if o1_text in procssed:\n",
        "                continue\n",
        "            else:\n",
        "                procssed.add(o1_text)\n",
        "\n",
        "            # o2_text has three sentences. We use DoLorean to generate one\n",
        "            # sentence at a time. See Appendix A.2 in the paper for more details.\n",
        "            o2_text_sents = extract_three_sentences(o2_text)\n",
        "\n",
        "            o2_text_so_far = \"\"\n",
        "            o1_text_so_far = \"\"\n",
        "            o1_addon = o1_text\n",
        "\n",
        "            for o2_sent in o2_text_sents:\n",
        "                o1_text_so_far = o1_text_so_far.strip() + \" \" + o1_addon.strip()\n",
        "                o2_text_so_far = o2_sent.strip()\n",
        "\n",
        "                # We want to ensure a space token between o1_text and o2_text\n",
        "                # during the decoding. To do so, here we append \". \" so that\n",
        "                # the GPT2 tokenizer later will not strip the space token. After\n",
        "                # tokenization, we delete the \".\" token.\n",
        "                o2_text_so_far = \". \" + o2_text_so_far\n",
        "\n",
        "                candidate_list = generate_counterfactual_story_endings(\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device,\n",
        "                    o1_text=o1_text_so_far,\n",
        "                    o2_text=o2_text_so_far,\n",
        "                    max_length=args.max_length,\n",
        "                    stepsize=args.stepsize,\n",
        "                    mix_rate=args.mix_rate,\n",
        "                    temperature_forward=args.temperature_forward,\n",
        "                    top_k=args.top_k,\n",
        "                    num_passes=args.num_passes,\n",
        "                    num_backward_iters=args.num_backward_iters,\n",
        "                    seed=args.seed,\n",
        "                    no_cuda=args.no_cuda,\n",
        "                    verbose=args.verbose)\n",
        "\n",
        "                d = {\n",
        "                    'premise': r['premise'],\n",
        "                    'initial': r['initial'],\n",
        "                    'counterfactual': r['counterfactual'],\n",
        "                    'original_ending': o2_text,\n",
        "                    'counterfactual_so_far': o1_text_so_far,\n",
        "                    'original_ending_so_far': o2_text_so_far,\n",
        "                    'H_Candidates': candidate_list\n",
        "                }\n",
        "                fw.write(json.dumps(d) + '\\n')\n",
        "                fw_txt.write(candidate_list[-1] + '\\n')\n",
        "\n",
        "                o1_addon = candidate_list[-1]  # pick the last candidate"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}