{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pieces.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNApahDP97RU875fO3yevae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/deep_unsupervised_learning/blob/master/pieces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-9AJNQh6kvR"
      },
      "source": [
        "\"\"\"\n",
        "Rank counterfactual candidates with BERT\n",
        "\"\"\"\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
        "BIG_CONST = 1e10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnFEdnbZEppX"
      },
      "source": [
        "Alternative: MegatronBert\n",
        "\n",
        "This is the only reasonable huggingface next sentence prediction task left "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4w-y0hXEmkd"
      },
      "source": [
        "# from transformers import MegatronBertForNextSentencePrediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOWR5pcLEtIa"
      },
      "source": [
        "Alternative: DistilBert "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqRWtToPEjzt"
      },
      "source": [
        "# from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Juytay7MQst"
      },
      "source": [
        "Alternative: Albert\n",
        "\n",
        "Here we use next sentence order prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGMhB0sYMQEA"
      },
      "source": [
        "#from transformers import AlbertForSequenceClassification,AlbertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZvWw2Aw9JIb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20wkxlL96phS"
      },
      "source": [
        "def read_original_data(fname, num_lines=-1):\n",
        "    \"\"\"\n",
        "    Read original data. Only read premise + counterfactual\n",
        "    as keys.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(fname, 'r') as fr:\n",
        "        lines = fr.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "\n",
        "            if i == num_lines:\n",
        "                break\n",
        "\n",
        "            line = line.strip()\n",
        "            line = json.loads(line)\n",
        "\n",
        "            data.append(' '.join([line['premise'], line['counterfactual']]))\n",
        "\n",
        "    print('Original data size: ', len(data))\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgw0dkP6tIv"
      },
      "source": [
        "def read_hyps_from_json_counterfactual(fname, num_lines=-1, hyps_dict=None):\n",
        "    \"\"\"\n",
        "    Read counterfactual reasoning hypotheses.\n",
        "\n",
        "    Since each counterfactual instance was splitted into 3 segments because the\n",
        "    story ending contains 3 sentences (see Appendix A.2 in the paper for more\n",
        "    details), this reading function will concatenate the 3 segments together to\n",
        "    obtain a complete instance.\n",
        "    \n",
        "    Example:\n",
        "    \n",
        "    \"counterfactual\": \"We are Washingtonians and rooted for the Seahawks.\", \n",
        "    \"original_ending\": \"In the final seconds I was looking at the floor depressed. \n",
        "    I was shocked and happy when my wife said the Patriots intercepted. \n",
        "    It was a great moment in Boston sports.\"}\n",
        "    \"\"\"\n",
        "    if num_lines > 0:\n",
        "        assert num_lines % 3 == 0\n",
        "\n",
        "    if hyps_dict is None:\n",
        "        hyps_dict = {}\n",
        "\n",
        "    print(\"Reading \", fname)\n",
        "\n",
        "    with open(fname, 'r') as fr:\n",
        "        lines = fr.readlines()\n",
        "        assert len(lines) % 3 == 0 or len(lines) > num_lines\n",
        "\n",
        "        for i in range(len(lines) // 3):\n",
        "\n",
        "            if i == num_lines // 3:\n",
        "                break\n",
        "\n",
        "            orig = []\n",
        "            hyps = []\n",
        "            for j in range(3):\n",
        "                line = json.loads(lines[i*3+j])\n",
        "\n",
        "                if j == 0:\n",
        "                    # Use premise + counterfactual as the key\n",
        "                    k = line[\"premise\"] + ' ' + line[\"counterfactual\"]\n",
        "                    if k not in hyps_dict:\n",
        "                        dict_k = {}\n",
        "                        dict_k[\"premise\"] = line[\"premise\"].strip()\n",
        "                        dict_k[\"initial\"] = line[\"initial\"].strip()\n",
        "                        dict_k[\"counterfactual\"] = line[\"counterfactual\"].strip()\n",
        "                        dict_k[\"original_ending\"] = line[\"original_ending\"].strip()\n",
        "                        dict_k[\"hyps_str\"] = {} # for replacing repeat entries\n",
        "                        dict_k[\"hyps\"] = []\n",
        "                        dict_k[\"orig\"] = []\n",
        "                        hyps_dict[k] = dict_k\n",
        "                    else:\n",
        "                        dict_k = hyps_dict[k]\n",
        "\n",
        "                orig_j = line[\"original_ending_so_far\"][len(\". \"):].strip()\n",
        "                orig.append(orig_j)\n",
        "\n",
        "                hyps_j = line[\"H_Candidates\"][-1].strip()\n",
        "                hyps.append(hyps_j)\n",
        "\n",
        "            # Replace repeat entries\n",
        "            hyps_str = ' '.join(hyps).strip()\n",
        "            if hyps_str in dict_k[\"hyps_str\"]:\n",
        "                hyps = ['DEPRECATED', '', '']\n",
        "            else:\n",
        "                dict_k[\"hyps_str\"][hyps_str] = 1\n",
        "\n",
        "            dict_k[\"hyps\"].append(hyps)\n",
        "            dict_k[\"orig\"].append(orig)\n",
        "\n",
        "    return hyps_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z44ONDoc6w7r"
      },
      "source": [
        "def is_deprecated(hyp):\n",
        "    return hyp[0] == 'DEPRECATED'\n",
        "\n",
        "\n",
        "def _has_repeat_sent(hyp):\n",
        "    \"\"\"\n",
        "    Detect if the sentences in `hyp` are repeat.\n",
        "\n",
        "    Args:\n",
        "        hyp: A list of three sentences.\n",
        "    \"\"\"\n",
        "    if len(hyp) <= 1:\n",
        "        return False\n",
        "\n",
        "    for i in range(1, len(hyp)):\n",
        "        a = hyp[i-1]\n",
        "        b = hyp[i]\n",
        "\n",
        "        if a == b:\n",
        "            return True\n",
        "\n",
        "        s = SequenceMatcher(None, a, b)\n",
        "        if len(a) > 5 and len(b) > 5 and s.ratio() >= 0.85:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def _has_repeat_substring(s, MINLEN=5, MINCNT=4):\n",
        "    d = {}\n",
        "    has_repeat = False\n",
        "    for sublen in range(int(len(s)/MINCNT)-1, MINLEN-1, -1):\n",
        "        for i in range(0, len(s)-sublen):\n",
        "            sub = s[i:i+sublen]\n",
        "            if len(sub.strip()) < sublen:\n",
        "                continue\n",
        "            cnt = s.count(sub)\n",
        "            if cnt >= MINCNT and sub not in d:\n",
        "                 d[sub] = cnt\n",
        "                 has_repeat = True\n",
        "                 break\n",
        "        if has_repeat:\n",
        "            break\n",
        "    return has_repeat\n",
        "\n",
        "\n",
        "def has_repeat(hyp):\n",
        "    \"\"\"\n",
        "    Detect if the hypothesis text has repeat patterns.\n",
        "    \"\"\"\n",
        "    has_repeat_substring = False\n",
        "    for h in hyp:\n",
        "        has_repeat_substring = has_repeat_substring or _has_repeat_substring(h)\n",
        "    return has_repeat_substring or _has_repeat_sent(hyp)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFBrcT2u60kq"
      },
      "source": [
        "def _get_bert_score(x, y, model, tokenizer, device='cuda'):\n",
        "    encoded = tokenizer.encode_plus(x, text_pair=y, return_tensors='pt')\n",
        "    for k in encoded:\n",
        "        encoded[k] = encoded[k].to(device)\n",
        "    seq_relationship_logits = model(**encoded)[0]\n",
        "    return seq_relationship_logits[0, 0].tolist()\n",
        "\n",
        "\n",
        "def rank_by_bert(ins, model, tokenizer, device='cuda'):\n",
        "    scores = []\n",
        "    for hyp in ins['hyps']:\n",
        "\n",
        "        if is_deprecated(hyp) or has_repeat(hyp):\n",
        "            # If the hypothesis has obvious repeat text, then\n",
        "            # discard it by setting the ranking scores to -1.\n",
        "            s = [-1] * 3\n",
        "            scores.append(s)\n",
        "            continue\n",
        "\n",
        "        o1_h1_score = _get_bert_score(\n",
        "            ins['premise'] + ' ' + ins['counterfactual'],\n",
        "            hyp[0],\n",
        "            model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "        o1_h123_score = _get_bert_score(\n",
        "            ins['premise'] + ' ' + ins['counterfactual'],\n",
        "            ' '.join(hyp),\n",
        "            model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "        h1_h2_score = _get_bert_score(\n",
        "            hyp[0], hyp[1],\n",
        "            model=model, tokenizer=tokenizer, device=device)\n",
        "        h2_h3_score = _get_bert_score(\n",
        "            hyp[1], hyp[2],\n",
        "            model=model, tokenizer=tokenizer, device=device)\n",
        "        avg_consec_score = np.mean([o1_h1_score, h1_h2_score, h2_h3_score])\n",
        "\n",
        "        s = [o1_h1_score, o1_h123_score, avg_consec_score]\n",
        "\n",
        "        scores.append(s)\n",
        "\n",
        "    return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i79UhypJ68Zl"
      },
      "source": [
        "def get_best_hyp(ins, scores):\n",
        "    # Rank by avg_consec_scores\n",
        "    acs = [s[2] for s in scores]\n",
        "    acs_ranked_indexes = sorted(range(len(acs)), key=lambda k: acs[k])\n",
        "    acs_ranked_indexes.reverse()\n",
        "    # Pick top K and rank by o1_h123\n",
        "    K = 2\n",
        "    o1_h123_scores = [scores[i][1] for i in acs_ranked_indexes[:K]]\n",
        "    best_o1_h123_scores_index = o1_h123_scores.index(max(o1_h123_scores))\n",
        "    best_index = acs_ranked_indexes[best_o1_h123_scores_index]\n",
        "\n",
        "    best_score = scores[best_index][2]\n",
        "    if best_score == -1:\n",
        "        return None\n",
        "    else:\n",
        "        return ins[\"hyps\"][best_index]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilitc1006-g-"
      },
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--hyps_dir\", type=str, default='./',\n",
        "                        help=\"Input dir, which contains josn files of hypotheses to be ranked.\")\n",
        "    parser.add_argument(\"--hyps_num_lines\", type=int, default=-1,\n",
        "                        help=\"The number of lines to be processed in the hypotheses files.\"\n",
        "                             \"For counterfactual reasoning, this number should be a multiple of 3.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default='./outputs', help=\"Output dir\")\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"no cuda\")\n",
        "    parser.add_argument(\"--align_to_original_data\", action=\"store_true\",\n",
        "                        help=\"Whether to align the output to the original input data.\")\n",
        "    parser.add_argument(\"--original_data_file\", type=str, default='../data/counterfactual/small_data.json',\n",
        "                        help=\"Filename of the original input data for counterfactual reasoning.\")\n",
        "    parser.add_argument(\"--original_num_lines\", type=int, default=-1,\n",
        "                        help=\"The number of lines (instances) to be read from the original data file.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Set the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "\n",
        "    def _maybe_create_dir(dirname):\n",
        "        \"\"\"Creates directory if doesn't exist\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(dirname):\n",
        "            os.makedirs(dirname)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    output_dir = args.output_dir\n",
        "    _maybe_create_dir(output_dir)\n",
        "\n",
        "    # Read hypotheses\n",
        "    if os.path.isfile(args.hyps_dir):\n",
        "        fnames = [args.hyps_dir]\n",
        "    else:\n",
        "        fnames = []\n",
        "        for f in os.listdir(args.hyps_dir):\n",
        "            fname = os.path.join(args.hyps_dir, f)\n",
        "            if os.path.isfile(fname) and fname.endswith('.json'):\n",
        "                fnames.append(fname)\n",
        "\n",
        "    hyps_dict = None\n",
        "    for fname in fnames:\n",
        "        hyps_dict = read_hyps_from_json_counterfactual(\n",
        "            fname, num_lines=args.hyps_num_lines, hyps_dict=hyps_dict)\n",
        "\n",
        "    if args.align_to_original_data:\n",
        "        original_data = read_original_data(args.original_data_file, num_lines=args.original_num_lines)\n",
        "        assert len(original_data) == len(hyps_dict)\n",
        "\n",
        "    # Load pretrained model\n",
        "    model = BertForNextSentencePrediction.from_pretrained('bert-base-cased') # change\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased') # change \n",
        "\n",
        "    # Rank\n",
        "    scores_dict = {}\n",
        "    for key, ins in tqdm.tqdm(hyps_dict.items()):\n",
        "        bert_scores = rank_by_bert(ins, model, tokenizer, device=device)\n",
        "        scores_dict[key] = bert_scores\n",
        "\n",
        "    # Output ranking scores\n",
        "    with open(output_dir + \"/scores.tsv\", \"w\") as fout:\n",
        "\n",
        "        fout.write('\\t'.join([\"premise\", \"initial\", \"counterfactual\", \"original_ending\",\n",
        "                              \"generated_ending\", \"bert_o1h1\", \"bert_o1h123\", \"bert_consec\"]) + '\\n')\n",
        "\n",
        "        def _write_ins(key, ins):\n",
        "            \"\"\"Write the scores of an instance.\"\"\"\n",
        "            for i, hyp in enumerate(ins[\"hyps\"]):\n",
        "                gen_ending = ' '.join(hyp).strip()\n",
        "                s = scores_dict[key][i]\n",
        "                fout.write('\\t'.join([ins[\"premise\"], ins[\"initial\"], ins[\"counterfactual\"],\n",
        "                                      ins[\"original_ending\"], gen_ending] +\n",
        "                                     [str(s_) for s_ in s]))\n",
        "                fout.write('\\n')\n",
        "\n",
        "        if args.align_to_original_data:\n",
        "            for key in original_data:\n",
        "                ins = hyps_dict[key]\n",
        "                _write_ins(key, ins)\n",
        "        else:\n",
        "            for key, ins in hyps_dict.items():\n",
        "                _write_ins(key, ins)\n",
        "\n",
        "\n",
        "    if args.align_to_original_data:\n",
        "        # Print ranked results\n",
        "        with open(output_dir + \"/ranked_samples.tsv\", \"w\") as fo_rank:\n",
        "\n",
        "            fo_rank.write('\\t'.join([\"premise\", \"initial\", \"counterfactual\", \"original_ending\",\n",
        "                                     \"ranked_generated_ending\"]) + '\\n')\n",
        "\n",
        "            for key in original_data:\n",
        "                ins = hyps_dict[key]\n",
        "\n",
        "                best_hyp = get_best_hyp(ins, scores_dict[key])\n",
        "                if best_hyp is None:\n",
        "                    # If all hyps are deprecated, then use the original ending\n",
        "                    gen_ending = ins[\"original_ending\"]\n",
        "                else:\n",
        "                    gen_ending = ' '.join(best_hyp).strip()\n",
        "\n",
        "                fo_rank.write('\\t'.join([ins[\"premise\"], ins[\"initial\"], ins[\"counterfactual\"],\n",
        "                                        ins[\"original_ending\"], gen_ending]))\n",
        "                fo_rank.write('\\n')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}